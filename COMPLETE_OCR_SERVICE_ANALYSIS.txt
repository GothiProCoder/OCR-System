
================================================================================
COMPREHENSIVE OCR_SERVICE.PY ANALYSIS - ALL ISSUES & SOLUTIONS
================================================================================

Reading through the entire codebase line by line...

================================================================================
SECTION 1: ARCHITECTURE & DESIGN ISSUES
================================================================================

ISSUE #1: Force CPU Mode (Line ~198-199)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

This COMPLETELY disables GPU usage for the entire Python process, even if user
has a capable GPU. This is set BEFORE checking if GPU is available or needed.

IMPACT: Critical
- Users with GPUs are forced to use slow CPU inference (~1.2s vs 120ms)
- No automatic fallback - it's hardcoded to CPU-only
- Affects entire process, not just this model

SOLUTION:
Remove this line completely. Let device_map="auto" handle device selection.
Only force CPU if explicitly configured.

FIXED CODE:
    # Remove: os.environ["CUDA_VISIBLE_DEVICES"] = ""

    # Instead, add to config:
    if settings.OCR_FORCE_CPU_ONLY:  # Only if user explicitly wants CPU
        os.environ["CUDA_VISIBLE_DEVICES"] = ""


ISSUE #2: Broken Device Auto-Detection (Lines 196-218)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
The code doesn't detect available hardware before deciding loading strategy.
It jumps straight to disk offload without checking:
1. Is GPU available?
2. Is GPU memory sufficient?
3. Is CPU RAM sufficient?

IMPACT: High
- No intelligent fallback based on actual hardware
- Always triggers slow disk offload path on low RAM systems
- Doesn't utilize GPU even when available

SOLUTION:
Add proper hardware detection first, then choose appropriate loading strategy.

FIXED CODE:
    import torch
    import psutil

    def _detect_hardware(self) -> dict:
        """Detect available hardware and capabilities"""
        gpu_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if gpu_available else 0

        # Get GPU memory if available
        gpu_memory = 0
        if gpu_available:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB

        # Get CPU RAM
        cpu_ram = psutil.virtual_memory().total / (1024**3)  # GB
        cpu_available = psutil.virtual_memory().available / (1024**3)  # GB

        return {
            'gpu_available': gpu_available,
            'gpu_count': gpu_count,
            'gpu_memory_gb': gpu_memory,
            'cpu_ram_gb': cpu_ram,
            'cpu_available_gb': cpu_available
        }

    def _choose_loading_strategy(self, hw_info: dict) -> str:
        """Choose best loading strategy based on hardware"""
        # Chandra needs ~2.9GB VRAM in FP16, ~1.9GB in INT8
        # CPU mode needs ~8-10GB RAM

        if hw_info['gpu_available'] and hw_info['gpu_memory_gb'] >= 6:
            return 'gpu_fp16'  # Best option
        elif hw_info['gpu_available'] and hw_info['gpu_memory_gb'] >= 3:
            return 'gpu_int8'  # Quantized
        elif hw_info['cpu_available_gb'] >= 10:
            return 'cpu_full'  # CPU with full precision
        elif hw_info['cpu_available_gb'] >= 6:
            return 'cpu_optimized'  # CPU with optimizations
        else:
            return 'cpu_offload'  # Last resort - disk offload


ISSUE #3: Singleton Pattern Implementation Flaw (Lines 127-147)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
The singleton uses _initialized flag but doesn't properly handle initialization
failures. If model loading fails, _initialized is already True, so retry won't work.

IMPACT: Medium
- After failed load, service is stuck in broken state
- Requires process restart to retry
- No recovery mechanism

SOLUTION:
Set _initialized = True only AFTER successful model load, not at the end of __init__.

FIXED CODE:
    def __init__(self):
        """Initialize service (only runs once due to singleton)"""
        if self._initialized:
            return

        self._manager = None
        self._model_loaded = False
        self._model_lock = threading.Lock()
        self._executor = ThreadPoolExecutor(max_workers=4)  # Increase from 2

        # Configuration
        self.max_dimension = settings.OCR_MAX_IMAGE_DIMENSION
        self.inference_method = settings.OCR_INFERENCE_METHOD

        # Don't set _initialized here, set it after successful model load
        logger.info(f"OCR Service initialized (method={self.inference_method})")

        # Now set initialized
        self._initialized = True


ISSUE #4: ThreadPoolExecutor Size (Line 148)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    self._executor = ThreadPoolExecutor(max_workers=2)

Only 2 workers means only 2 concurrent OCR requests can be processed.
With CPU inference taking ~1.2s/page, this becomes a bottleneck.

IMPACT: Medium
- Poor concurrency
- Requests queue up quickly
- Underutilizes multi-core CPUs

SOLUTION:
Increase to at least CPU count / 2, or configurable.

FIXED CODE:
    import os
    max_workers = min(os.cpu_count() or 4, 8)  # 4-8 workers
    self._executor = ThreadPoolExecutor(max_workers=max_workers)


================================================================================
SECTION 2: MODEL LOADING ISSUES
================================================================================

ISSUE #5: Incorrect InferenceManager Usage (Lines 200-206)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Code tries InferenceManager without checking if it supports CPU-only mode or
if it will work with current hardware. InferenceManager might internally
try to load to GPU and fail.

IMPACT: High
- Causes immediate fallback to broken disk offload
- No proper error handling for expected failures
- Doesn't pass device parameter to InferenceManager

SOLUTION:
Pass device information to InferenceManager or catch specific exceptions.

FIXED CODE:
    try:
        from chandra.model import InferenceManager

        # Determine device based on hardware detection
        hw_info = self._detect_hardware()
        device = 'cuda' if hw_info['gpu_available'] else 'cpu'

        # Try InferenceManager with explicit device
        self._manager = InferenceManager(
            method=self.inference_method,
            device=device  # Explicitly pass device
        )
        self._use_direct_model = False
        logger.info(f"Model loaded with InferenceManager on {device}")

    except (RuntimeError, ValueError, torch.cuda.OutOfMemoryError) as e:
        # Expected failures - try alternative loading
        logger.warning(f"InferenceManager failed: {e}")
        logger.info(f"Attempting alternative loading strategy...")
        self._load_with_alternative_strategy(hw_info)


ISSUE #6: CRITICAL - Broken Disk Offload Implementation (Lines 220-295)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Multiple critical bugs in _load_model_with_disk_offload():

A. Using AutoModel.from_config() (Line 290):
   - AutoModel doesn't know about Qwen3VLForConditionalGeneration
   - Requires trust_remote_code=True but AutoModel.from_config doesn't support it
   - Creates generic model structure, not Chandra's specific architecture

B. Wrong checkpoint parameter (Line 295):
   - Passes directory path (model_path) to checkpoint parameter
   - load_checkpoint_and_dispatch expects file path to .safetensors/.bin
   - Should be: checkpoint=str(model_path / "model.safetensors")

C. Device map issue:
   - device_map="auto" with offload_folder should work
   - But init_empty_weights() + AutoModel breaks the flow

D. Missing error handling for safetensors device error

IMPACT: CRITICAL - This is the root cause of your error
- "device disk is invalid" error
- Complete failure to load model
- Service is unusable

SOLUTION:
Don't use init_empty_weights() + load_checkpoint_and_dispatch for this.
Use from_pretrained() with offload_folder directly.

FIXED CODE:
    def _load_model_with_disk_offload(self, offload_dir: Path) -> None:
        """
        Load model with proper disk offload for extreme low-memory systems.
        Uses from_pretrained with offload_folder parameter.
        """
        import torch
        from transformers import AutoProcessor

        # Import correct model class
        try:
            from transformers import Qwen2VLForConditionalGeneration
            ModelClass = Qwen2VLForConditionalGeneration
        except ImportError:
            # Fallback
            from transformers import AutoModelForVision2Seq
            ModelClass = AutoModelForVision2Seq

        model_checkpoint = "datalab-to/chandra"

        logger.info("Loading with disk offload (SLOW - 10-20 mins first time)...")

        # Load processor (lightweight)
        self._processor = AutoProcessor.from_pretrained(
            model_checkpoint,
            trust_remote_code=True
        )

        # Load model with proper offload
        # DO NOT use init_empty_weights + load_checkpoint_and_dispatch
        # Use from_pretrained with offload_folder instead
        self._model = ModelClass.from_pretrained(
            model_checkpoint,
            device_map="auto",  # Let accelerate decide placement
            offload_folder=str(offload_dir),  # Offload to disk
            offload_state_dict=True,  # Offload optimizer state too
            trust_remote_code=True,  # REQUIRED for Chandra
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True  # Minimize CPU RAM during load
        )

        self._model.processor = self._processor
        logger.info("Model loaded with disk offload")


ISSUE #7: Quantization Only Works on GPU (Not mentioned in code)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
The recommended fix (8-bit quantization) using bitsandbytes ONLY works on GPU.
If user has CPU-only system, quantization will fail.

IMPACT: High
- Quantization solution breaks on CPU-only systems
- bitsandbytes requires CUDA
- No CPU-compatible quantization method provided

SOLUTION:
Add CPU-compatible quantization using different methods, or skip quantization on CPU.

FIXED CODE:
    def _load_optimized_cpu(self) -> None:
        """Load model optimized for CPU inference (no GPU)"""
        import torch
        from transformers import AutoProcessor

        try:
            from transformers import Qwen2VLForConditionalGeneration
            ModelClass = Qwen2VLForConditionalGeneration
        except ImportError:
            from transformers import AutoModelForVision2Seq
            ModelClass = AutoModelForVision2Seq

        model_checkpoint = "datalab-to/chandra"

        logger.info("Loading model for CPU inference...")

        # Load processor
        self._processor = AutoProcessor.from_pretrained(
            model_checkpoint,
            trust_remote_code=True
        )

        # Load model to CPU with optimizations
        # DO NOT use bitsandbytes quantization (GPU-only)
        self._model = ModelClass.from_pretrained(
            model_checkpoint,
            torch_dtype=torch.float32,  # CPU works better with float32
            device_map="cpu",  # Explicit CPU
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        # Optional: Convert to channels_last for better CPU performance
        self._model = self._model.to(memory_format=torch.channels_last)

        # Enable CPU optimizations
        if hasattr(torch, 'set_num_threads'):
            torch.set_num_threads(os.cpu_count() or 4)

        self._model.processor = self._processor
        logger.info(f"Model loaded on CPU (threads={torch.get_num_threads()})")


ISSUE #8: No GPU Quantization Path (Missing implementation)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Code doesn't have a GPU quantization path. Only has broken disk offload.

IMPACT: High
- Users with GPUs but limited VRAM can't use quantization
- Miss 35% memory reduction opportunity
- Slower performance than necessary

SOLUTION:
Add proper GPU quantization method.

FIXED CODE:
    def _load_quantized_gpu(self, bits: int = 8) -> None:
        """Load model with quantization on GPU"""
        import torch
        from transformers import AutoProcessor, BitsAndBytesConfig

        try:
            from transformers import Qwen2VLForConditionalGeneration
            ModelClass = Qwen2VLForConditionalGeneration
        except ImportError:
            from transformers import AutoModelForVision2Seq
            ModelClass = AutoModelForVision2Seq

        model_checkpoint = "datalab-to/chandra"

        logger.info(f"Loading model with {bits}-bit quantization on GPU...")

        # Load processor
        self._processor = AutoProcessor.from_pretrained(
            model_checkpoint,
            trust_remote_code=True
        )

        # Configure quantization
        if bits == 8:
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0
            )
        elif bits == 4:
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )
        else:
            raise ValueError(f"Unsupported quantization bits: {bits}")

        # Load quantized model
        self._model = ModelClass.from_pretrained(
            model_checkpoint,
            quantization_config=quant_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )

        self._model.processor = self._processor
        logger.info(f"Model loaded with {bits}-bit quantization")


================================================================================
SECTION 3: INFERENCE & PROCESSING ISSUES
================================================================================

ISSUE #9: Missing Error Handling in _generate_with_direct_model (Lines 305-335)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Method assumes chandra.model.hf and chandra.model.schema exist and work.
No fallback if these fail or model is loaded differently.

IMPACT: Medium
- Crashes if chandra's internal APIs change
- No fallback for direct PyTorch inference
- Doesn't handle processor attachment errors

SOLUTION:
Add proper error handling and fallback to manual inference.

(See previously generated _generate_with_direct_model and _manual_inference)


ISSUE #10: No Batch Processing Support (Entire file)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Every image is processed individually, even when processing PDFs with multiple
pages. No batching support.

IMPACT: Medium
- Much slower than necessary for multi-page PDFs
- Doesn't utilize GPU batch processing capabilities
- CPU: Could process 2-4 pages in parallel

SOLUTION:
Add batch processing for multi-page documents.

FIXED CODE:
    def _process_batch_sync(
        self,
        images: List[Image.Image],
        page_numbers: List[int]
    ) -> List[OCROutput]:
        """Process multiple images in a batch"""
        self._ensure_model_loaded()

        if len(images) == 0:
            return []

        # Optimize batch size based on available memory
        hw_info = self._detect_hardware()
        if hw_info['gpu_available']:
            batch_size = 8 if hw_info['gpu_memory_gb'] >= 8 else 4
        else:
            batch_size = 2  # CPU: smaller batches

        results = []
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            batch_pages = page_numbers[i:i+batch_size]

            # Process batch
            batch_results = self._process_single_batch(batch_images, batch_pages)
            results.extend(batch_results)

        return results

    def _process_single_batch(
        self,
        images: List[Image.Image],
        page_numbers: List[int]
    ) -> List[OCROutput]:
        """Process a single batch of images"""
        start_time = time.time()

        try:
            # Optimize all images
            optimized_images = [
                image_preprocessor.resize_if_needed(img, self.max_dimension)
                for img in images
            ]

            # Create batch input
            from chandra.model.schema import BatchInputItem
            batch = [
                BatchInputItem(image=img, prompt_type="ocr_layout")
                for img in optimized_images
            ]

            # Generate for batch
            if hasattr(self, '_use_direct_model') and self._use_direct_model:
                # Direct model
                from chandra.model.hf import generate_hf
                batch_results = generate_hf(batch, self._model)
            else:
                # InferenceManager
                batch_results = self._manager.generate(batch)

            # Convert to OCROutput
            outputs = []
            for idx, result in enumerate(batch_results):
                processing_time = int((time.time() - start_time) * 1000 / len(images))
                outputs.append(OCROutput(
                    markdown=result.markdown or "",
                    html=result.html or "",
                    json_output=result.json if hasattr(result, 'json') else {},
                    processing_time_ms=processing_time,
                    success=True,
                    page_number=page_numbers[idx],
                    image_width=images[idx].size[0],
                    image_height=images[idx].size[1]
                ))

            return outputs

        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            # Fallback to individual processing
            return [
                self._process_single_image_sync(img, page_num)
                for img, page_num in zip(images, page_numbers)
            ]


ISSUE #11: Redundant Image Optimization (Lines 425-430)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    # In process_image_sync:
    image = image_preprocessor.optimize_for_ocr(image, ...)

    # Then in _process_single_image_sync:
    optimized_image = image_preprocessor.resize_if_needed(image, ...)

If optimize_for_ocr already resizes, this is redundant.

IMPACT: Low
- Slight performance overhead
- Potential double-processing

SOLUTION:
Check if optimize_for_ocr includes resizing, or do all optimization in one place.

FIXED CODE:
    def process_image_sync(self, image_source, page_number=1) -> OCROutput:
        # Load image
        if isinstance(image_source, bytes):
            image = image_preprocessor.load_image_bytes(image_source)
        elif isinstance(image_source, (str, Path)):
            image = image_preprocessor.load_image(image_source)
        elif isinstance(image_source, Image.Image):
            image = image_source
        else:
            raise ValueError(f"Unsupported image source type: {type(image_source)}")

        # Do ALL preprocessing in one place
        image = image_preprocessor.preprocess_for_chandra(
            image,
            max_dimension=self.max_dimension,
            apply_contrast=True,
            apply_sharpness=True
        )

        return self._process_single_image_sync(image, page_number)


ISSUE #12: No Timeout on Synchronous Processing (Lines 340-390)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Sync processing methods have no timeout. With disk offload taking 5-10 minutes,
a single request can hang indefinitely.

IMPACT: Medium
- Requests can hang forever
- No way to detect stuck processing
- Blocks thread pool workers

SOLUTION:
Add timeout using signal (Unix) or threading.Timer (cross-platform).

FIXED CODE:
    import signal
    from contextlib import contextmanager

    class TimeoutException(Exception):
        pass

    @contextmanager
    def time_limit(seconds):
        """Context manager for timeout"""
        def signal_handler(signum, frame):
            raise TimeoutException("Processing timed out")

        # Set alarm
        signal.signal(signal.SIGALRM, signal_handler)
        signal.alarm(seconds)
        try:
            yield
        finally:
            signal.alarm(0)

    def _process_single_image_sync(self, image, page_number=1) -> OCROutput:
        start_time = time.time()
        original_width, original_height = image.size

        try:
            # Add timeout (5 minutes for disk offload)
            with time_limit(300):
                self._ensure_model_loaded()

                optimized_image = image_preprocessor.resize_if_needed(
                    image, self.max_dimension
                )

                # ... rest of processing ...

        except TimeoutException:
            processing_time = int((time.time() - start_time) * 1000)
            logger.error("OCR processing timed out")
            return OCROutput(
                markdown="",
                html="",
                json_output={},
                processing_time_ms=processing_time,
                success=False,
                error="Processing timed out after 300 seconds",
                page_number=page_number,
                image_width=original_width,
                image_height=original_height
            )
        except Exception as e:
            # ... existing error handling ...


================================================================================
SECTION 4: ASYNC & CONCURRENCY ISSUES
================================================================================

ISSUE #13: No Timeout in Async Methods (Lines 470-510)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    async def process_image(self, image_source, page_number=1):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor,
            self.process_image_sync,
            image_source,
            page_number
        )

No timeout wrapper. Can hang indefinitely.

IMPACT: Medium
- FastAPI requests hang
- No user feedback
- Ties up thread pool

SOLUTION:
Already provided in previous response - use asyncio.wait_for().


ISSUE #14: get_event_loop() Deprecation (Lines 475, 500)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    loop = asyncio.get_event_loop()

Deprecated in Python 3.10+. Should use get_running_loop().

IMPACT: Low
- Deprecation warnings
- May break in future Python versions

SOLUTION:
Use get_running_loop() or asyncio.to_thread().

FIXED CODE:
    async def process_image(self, image_source, page_number=1, timeout=300.0):
        try:
            # Python 3.9+: Use asyncio.to_thread
            return await asyncio.wait_for(
                asyncio.to_thread(
                    self.process_image_sync,
                    image_source,
                    page_number
                ),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            logger.error(f"Processing timed out after {timeout}s")
            return OCROutput(
                success=False,
                error=f"Processing timed out after {timeout} seconds"
            )


ISSUE #15: No Async Context Manager for Cleanup (Missing feature)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
No way to properly initialize and cleanup in FastAPI lifespan events.
ThreadPoolExecutor is never properly shut down.

IMPACT: Low
- Resource leaks on server shutdown
- Threads may not terminate cleanly

SOLUTION:
Add async context manager support.

FIXED CODE:
    async def __aenter__(self):
        """Async context manager entry"""
        self.preload_model()  # Preload during startup
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.cleanup_async()

    async def cleanup_async(self):
        """Async cleanup method"""
        # Shutdown executor
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=True, cancel_futures=True)

        # Cleanup model in thread
        await asyncio.to_thread(self.cleanup_model)


================================================================================
SECTION 5: MEMORY & RESOURCE MANAGEMENT
================================================================================

ISSUE #16: No Memory Cleanup After Processing (Entire file)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Large images are processed but never explicitly cleaned up.
Python's GC may not run immediately, causing memory accumulation.

IMPACT: Medium
- Memory grows over time
- OOM on high request volume
- Especially bad with large PDFs

SOLUTION:
Explicit cleanup after processing.

FIXED CODE:
    def _process_single_image_sync(self, image, page_number=1) -> OCROutput:
        import gc

        start_time = time.time()
        original_width, original_height = image.size

        try:
            self._ensure_model_loaded()

            optimized_image = image_preprocessor.resize_if_needed(
                image, self.max_dimension
            )

            # ... processing ...

            result = OCROutput(...)

            # Explicit cleanup
            del optimized_image
            if image != optimized_image:
                del image
            gc.collect()

            return result

        except Exception as e:
            gc.collect()  # Cleanup even on error
            # ... error handling ...


ISSUE #17: No Model Unloading Method (Missing feature)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Once model is loaded, no way to unload it without restarting process.
Useful for:
- Memory pressure situations
- Switching models
- Recovery from failed loads

IMPACT: Medium
- Can't recover from memory issues
- Process restart required for model changes

SOLUTION:
Already provided - cleanup_model() method.


ISSUE #18: No Cache Management for HuggingFace Downloads (Line 267)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    model_path = snapshot_download(
        model_checkpoint,
        ignore_patterns=["*.md", "*.txt"]
    )

No cache_dir specified. On Windows, causes symlink warnings and inefficient storage.

IMPACT: Low-Medium
- Inefficient disk usage
- Multiple downloads of same model
- Windows symlink issues (your error log shows this)

SOLUTION:
Specify explicit cache directory.

FIXED CODE:
    from pathlib import Path

    cache_dir = Path.home() / ".cache" / "chandra_ocr"
    cache_dir.mkdir(parents=True, exist_ok=True)

    model_path = snapshot_download(
        model_checkpoint,
        cache_dir=str(cache_dir),
        ignore_patterns=["*.md", "*.txt"],
        local_files_only=False  # Allow downloads
    )


================================================================================
SECTION 6: ERROR HANDLING & LOGGING
================================================================================

ISSUE #19: Poor Error Messages (Multiple locations)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Error messages don't provide actionable information.
Example (Line 218):
    raise RuntimeError(f"Failed to load OCR model: {e}") from e

Doesn't tell user what to do or why it failed.

IMPACT: Low
- Users don't know how to fix issues
- Debugging is harder

SOLUTION:
Add actionable error messages.

FIXED CODE:
    except Exception as e:
        error_msg = f"""
        Failed to load Chandra OCR model: {e}

        Troubleshooting:
        1. Check if you have enough RAM (need at least 8GB available)
        2. If using GPU, check CUDA installation: nvidia-smi
        3. Try CPU-only mode: Set OCR_FORCE_CPU_ONLY=True
        4. For low RAM, install: pip install bitsandbytes

        Hardware detected:
        - GPU: {hw_info['gpu_available']}
        - GPU Memory: {hw_info.get('gpu_memory_gb', 0):.1f}GB
        - Available RAM: {hw_info['cpu_available_gb']:.1f}GB
        """
        logger.error(error_msg)
        raise RuntimeError(error_msg) from e


ISSUE #20: No Progress Indication for Long Operations (Lines 220-295)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Disk offload takes 10-20 minutes but no progress indication.
User sees nothing and thinks it's frozen.

IMPACT: Medium
- Poor UX
- Users kill process thinking it's stuck
- No visibility into what's happening

SOLUTION:
Add progress logging or callback.

FIXED CODE:
    logger.info("Loading model with disk offload...")
    logger.info("Progress: [1/4] Downloading model files...")
    model_path = snapshot_download(...)

    logger.info("Progress: [2/4] Loading processor...")
    self._processor = AutoProcessor.from_pretrained(...)

    logger.info("Progress: [3/4] Creating model structure...")
    logger.info("This step takes 10-20 minutes on first run. Please wait...")

    self._model = ModelClass.from_pretrained(...)

    logger.info("Progress: [4/4] Model loaded successfully!")


ISSUE #21: Exception Swallowing in Fallback Logic (Lines 196-218)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Broad exception catching hides real issues:
    except Exception as e:
        logger.error(f"Failed to load Chandra model: {e}")
        raise RuntimeError(f"Failed to load OCR model: {e}") from e

Should catch specific exceptions and handle differently.

IMPACT: Medium
- ImportError treated same as OOM error
- Wrong recovery strategy may be applied
- Debugging is harder

SOLUTION:
Catch specific exceptions and handle appropriately.

FIXED CODE:
    try:
        self._manager = InferenceManager(method=self.inference_method)

    except ImportError as e:
        # Missing dependency - clear error message
        raise ImportError(
            f"Chandra OCR not installed: {e}\n"
            f"Install with: pip install chandra-ocr"
        ) from e

    except torch.cuda.OutOfMemoryError as e:
        # OOM on GPU - try quantization or CPU
        logger.warning(f"GPU OOM: {e}")
        self._load_with_alternative_strategy(strategy='gpu_int8')

    except RuntimeError as e:
        if "memory" in str(e).lower():
            # Memory issue - try less memory-intensive method
            logger.warning(f"Memory error: {e}")
            self._load_with_alternative_strategy(strategy='cpu_full')
        else:
            raise

    except Exception as e:
        # Unexpected error - re-raise with context
        logger.error(f"Unexpected error loading model: {e}", exc_info=True)
        raise


================================================================================
SECTION 7: CONFIGURATION & FLEXIBILITY
================================================================================

ISSUE #22: Hard-coded Model Name (Multiple locations)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
    model_checkpoint = "datalab-to/chandra"

Hard-coded throughout. Can't easily switch to fine-tuned version or local model.

IMPACT: Low
- Not flexible for custom models
- Can't A/B test different versions

SOLUTION:
Move to configuration.

FIXED CODE:
    # In config.py
    OCR_MODEL_NAME = "datalab-to/chandra"
    OCR_MODEL_LOCAL_PATH = None  # Optional: local model path

    # In ocr_service.py
    model_checkpoint = (
        settings.OCR_MODEL_LOCAL_PATH or 
        settings.OCR_MODEL_NAME
    )


ISSUE #23: No Device Override Configuration (Missing)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
User can't easily force GPU or CPU mode via config.
Currently requires code changes.

IMPACT: Low
- Testing is harder
- Deployment flexibility reduced

SOLUTION:
Add configuration options.

FIXED CODE:
    # In config.py
    OCR_DEVICE = "auto"  # "auto", "cpu", "cuda", "cuda:0", etc.
    OCR_FORCE_CPU_ONLY = False
    OCR_USE_QUANTIZATION = True
    OCR_QUANTIZATION_BITS = 8  # 4 or 8
    OCR_ENABLE_BATCH_PROCESSING = True
    OCR_MAX_BATCH_SIZE = 8

    # In ocr_service.py
    def _get_device_from_config(self):
        if settings.OCR_DEVICE != "auto":
            return settings.OCR_DEVICE
        # Otherwise, auto-detect
        return "cuda" if torch.cuda.is_available() else "cpu"


ISSUE #24: No Warm-up/Health Check (Missing)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
First request takes very long due to model loading.
No health check endpoint to verify model is loaded.

IMPACT: Low-Medium
- Poor UX on first request
- Can't verify service readiness
- Health checks fail during startup

SOLUTION:
Add health check and warm-up.

FIXED CODE:
    def health_check(self) -> dict:
        """Health check for monitoring"""
        return {
            "status": "healthy" if self._model_loaded else "loading",
            "model_loaded": self._model_loaded,
            "hardware": self._detect_hardware() if self._model_loaded else None
        }

    def warmup(self):
        """Warm up model with dummy inference"""
        if not self._model_loaded:
            self.preload_model()

        # Create small dummy image
        dummy_image = Image.new('RGB', (100, 100), color='white')

        try:
            self._process_single_image_sync(dummy_image, page_number=0)
            logger.info("Model warm-up completed")
        except Exception as e:
            logger.warning(f"Warm-up failed: {e}")


================================================================================
SECTION 8: MISSING FEATURES
================================================================================

ISSUE #25: No Metrics/Monitoring (Missing)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
No metrics collection for:
- Processing times
- Success/failure rates
- Memory usage
- Request volume

IMPACT: Low
- Can't monitor performance
- No alerting on issues
- Can't optimize based on real usage

SOLUTION:
Add metrics collection.

FIXED CODE:
    from dataclasses import dataclass, field
    from collections import defaultdict
    import statistics

    @dataclass
    class ServiceMetrics:
        total_requests: int = 0
        successful_requests: int = 0
        failed_requests: int = 0
        processing_times: list = field(default_factory=list)
        total_pages_processed: int = 0

        def record_request(self, success: bool, processing_time_ms: int, pages: int = 1):
            self.total_requests += 1
            if success:
                self.successful_requests += 1
            else:
                self.failed_requests += 1
            self.processing_times.append(processing_time_ms)
            self.total_pages_processed += pages

            # Keep only last 1000 measurements
            if len(self.processing_times) > 1000:
                self.processing_times = self.processing_times[-1000:]

        def get_stats(self) -> dict:
            if not self.processing_times:
                return {}
            return {
                "total_requests": self.total_requests,
                "success_rate": self.successful_requests / self.total_requests if self.total_requests > 0 else 0,
                "avg_processing_time_ms": statistics.mean(self.processing_times),
                "p95_processing_time_ms": statistics.quantiles(self.processing_times, n=20)[18] if len(self.processing_times) > 20 else None,
                "total_pages": self.total_pages_processed
            }

    # In OCRService.__init__:
    self._metrics = ServiceMetrics()

    # Add to process methods:
    def process_image_sync(self, ...):
        start = time.time()
        result = ... # processing
        processing_time = int((time.time() - start) * 1000)
        self._metrics.record_request(result.success, processing_time, pages=1)
        return result


ISSUE #26: No Rate Limiting (Missing)
────────────────────────────────────────────────────────────────────────────
PROBLEM:
Service can be overwhelmed with requests.
With CPU taking ~1.2s/page, queue grows quickly.

IMPACT: Medium
- Service degradation under load
- OOM from too many concurrent requests
- No backpressure mechanism

SOLUTION:
Add rate limiting or request queue.

FIXED CODE:
    from asyncio import Semaphore

    # In OCRService.__init__:
    self._max_concurrent = 4  # Max concurrent processing
    self._semaphore = Semaphore(self._max_concurrent)

    # Wrap async methods:
    async def process_image(self, ...):
        async with self._semaphore:
            # ... existing code ...


================================================================================
SUMMARY: ALL 26 ISSUES CATEGORIZED
================================================================================

CRITICAL (Must fix immediately):
├── #1: Force CPU mode disables GPU
├── #6: Broken disk offload implementation
└── #7: Quantization doesn't work on CPU

HIGH (Fix before production):
├── #2: No hardware auto-detection
├── #5: Incorrect InferenceManager usage
├── #8: Missing GPU quantization path
└── #21: Poor exception handling

MEDIUM (Fix soon):
├── #3: Singleton initialization flaw
├── #4: Small thread pool
├── #9: Missing error handling in generation
├── #10: No batch processing
├── #12: No timeout on sync processing
├── #13: No timeout on async processing
├── #16: No memory cleanup
├── #17: No model unloading
├── #19: Poor error messages
├── #20: No progress indication
└── #26: No rate limiting

LOW (Nice to have):
├── #11: Redundant image optimization
├── #14: Deprecated get_event_loop()
├── #15: No async context manager
├── #18: No cache management
├── #22: Hard-coded model name
├── #23: No device override config
├── #24: No warm-up/health check
└── #25: No metrics/monitoring

================================================================================
RECOMMENDED FIX ORDER
================================================================================

1. FIRST: Fix device detection (#1, #2)
   - Remove forced CPU mode
   - Add hardware detection
   - Choose loading strategy based on hardware

2. SECOND: Fix model loading (#6, #7, #8)
   - Fix disk offload implementation
   - Add GPU quantization path
   - Add optimized CPU path
   - Don't use bitsandbytes on CPU

3. THIRD: Fix error handling (#5, #21, #19)
   - Specific exception catching
   - Better error messages
   - Proper fallback logic

4. FOURTH: Add missing features (#10, #12, #13, #16)
   - Batch processing
   - Timeouts
   - Memory cleanup

5. FIFTH: Production readiness (#24, #25, #26)
   - Health checks
   - Metrics
   - Rate limiting

================================================================================
END OF ANALYSIS
================================================================================
